{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95da72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellcheck complete. Outputs saved.\n"
     ]
    }
   ],
   "source": [
    "omit_words = {\"passu\", \"annum\",\"exploramed\", \"issuances\", \"Facteau\", \"recapitalizations\", \"TECHNOLOGEES\", \"Laramore\", \"unvested\", \"pharma\"}\n",
    "dictionary_path = r\"D:\\vc-research\\vc-research\\frequency_dictionary_en_82_765.txt\"\n",
    "import os\n",
    "import re\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Initialize SymSpell\n",
    "max_edit_distance = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(max_edit_distance, prefix_length)\n",
    "\n",
    "# Load dictionary\n",
    "\n",
    "term_index = 0\n",
    "count_index = 1\n",
    "sym_spell.load_dictionary(dictionary_path, term_index, count_index)\n",
    "\n",
    "# Paths\n",
    "input_folder = r\"D:\\vc-research\\vc-research\\Batch56_text_readable\"\n",
    "output_folder = r\"D:\\vc-research\\vc-research\\batch56_spellcheck\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Logs\n",
    "correction_counts = []\n",
    "corrections_list = []\n",
    "\n",
    "# Words to skip (case-insensitive)\n",
    "\n",
    "\n",
    "# Token pattern: words, punctuation, spaces, tabs, line breaks\n",
    "token_pattern = re.compile(r\"(\\w+|\\s+|[^\\w\\s])\", re.UNICODE)\n",
    "\n",
    "# Process each file\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, f\"checked_{filename}\")\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "\n",
    "        tokens = token_pattern.findall(text)\n",
    "        corrected_tokens = []\n",
    "        correction_count = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.strip() == \"\" or not token.isalpha():\n",
    "                corrected_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            token_lower = token.lower()\n",
    "\n",
    "            # Skip correction for short words, omit list, or ALL CAPS\n",
    "            if len(token) <= 4 or token_lower in omit_words or token.isupper():\n",
    "                corrected_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            # Track casing\n",
    "            original_case = (\n",
    "                \"upper\" if token.isupper() else\n",
    "                \"title\" if token.istitle() else\n",
    "                \"lower\"\n",
    "            )\n",
    "\n",
    "            suggestions = sym_spell.lookup(token_lower, Verbosity.CLOSEST, max_edit_distance)\n",
    "            if suggestions and suggestions[0].term != token_lower:\n",
    "                corrected = suggestions[0].term\n",
    "                correction_count += 1\n",
    "\n",
    "                # Restore case\n",
    "                if original_case == \"upper\":\n",
    "                    corrected = corrected.upper()\n",
    "                elif original_case == \"title\":\n",
    "                    corrected = corrected.title()\n",
    "\n",
    "                corrections_list.append(f\"{filename} | {token} → {corrected}\")\n",
    "            else:\n",
    "                corrected = token\n",
    "\n",
    "            corrected_tokens.append(corrected)\n",
    "\n",
    "        # Save corrected file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(\"\".join(corrected_tokens))\n",
    "\n",
    "        correction_counts.append(f\"{filename}: {correction_count}\")\n",
    "\n",
    "# Save correction summary\n",
    "with open(os.path.join(output_folder, \"correctioncount.txt\"), \"w\", encoding=\"utf-8\") as summary_file:\n",
    "    summary_file.write(\"\\n\".join(correction_counts))\n",
    "\n",
    "# Save correction details\n",
    "with open(os.path.join(output_folder, \"correctionslist.txt\"), \"w\", encoding=\"utf-8\") as correction_file:\n",
    "    correction_file.write(\"\\n\".join(corrections_list))\n",
    "\n",
    "print(\"Spellcheck complete. Outputs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfb3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15485/15485 [01:35<00:00, 162.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tokens\n",
      "count   15485.000000\n",
      "mean     9555.540652\n",
      "std      7626.149323\n",
      "min        22.000000\n",
      "25%       864.000000\n",
      "50%     11280.000000\n",
      "75%     15536.000000\n",
      "max    110403.000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing your .txt files\n",
    "input_dir = r\"D:\\vc-research\\vc-research\\Batch56_text_readable\"\n",
    "\n",
    "# Choose the model you're planning to use\n",
    "MODEL_NAME = \"gpt-4\"  # or \"gpt-4o\" or \"gpt-3.5-turbo\"\n",
    "\n",
    "# Load tokenizer for selected model\n",
    "encoding = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "\n",
    "# Results container\n",
    "token_data = []\n",
    "\n",
    "# Loop over .txt files and estimate tokens\n",
    "for fname in tqdm(os.listdir(input_dir)):\n",
    "    if not fname.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(input_dir, fname)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    token_count = len(encoding.encode(text))\n",
    "    token_data.append({\"filename\": fname, \"tokens\": token_count})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tokens = pd.DataFrame(token_data)\n",
    "\n",
    "# Summary stats\n",
    "print(df_tokens.describe())\n",
    "# Pricing (GPT-4o July 2025 rates)\n",
    "input_price_per_1k = 0.005  # $5 per million tokens\n",
    "output_price_per_1k = 0.015 # assumed 500-token output\n",
    "\n",
    "df_tokens[\"estimated_output_tokens\"] = 500\n",
    "df_tokens[\"cost_usd\"] = (df_tokens[\"tokens\"] + df_tokens[\"estimated_output_tokens\"]) / 1000 * (input_price_per_1k + output_price_per_1k)\n",
    "\n",
    "# Save again with cost\n",
    "df_tokens.to_csv(\"token_estimates_with_cost.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
