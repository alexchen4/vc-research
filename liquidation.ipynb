{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liquidity Preference Classification & Extraction Pipeline\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook trains a classifier to identify which documents mention “liquidation preference,” then extracts and tags the specific sentences containing key properties (Company Name, Date, Document Type, Preferred Stocks, Priority Order, Liquidation Value).\n",
    "\n",
    "**Inputs:**  \n",
    "- Labeled CSVs:  \n",
    "  - `URAP VC Research - [Readable] Batch 1 Main.csv` (has a binary label “Contains Liquidity Preference”)  \n",
    "  - `URAP VC Research - Batch 1 Details.csv` (additional metadata)  \n",
    "- Plain-text `.txt` files converted from PDFs in `Batch1_text_readable`.\n",
    "\n",
    "**Outputs:**  \n",
    "1. A trained RandomForest classifier with TF-IDF + BERT embeddings.  \n",
    "2. A DataFrame of test-set predictions and confidence scores.  \n",
    "3. A fine-tuned Sentence-Transformer model for tagging sentences.  \n",
    "4. A CSV (`Extracted Sentences - Batch 1.csv`) listing, for each file, the sentences tagged with each property.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#setup)  \n",
    "2. [Paths & Data Loading](#data)  \n",
    "3. [DataFrame Preparation](#prep)  \n",
    "4. [Text Preprocessing & Label Loading](#text)  \n",
    "5. [Document Classification Pipeline](#classify)  \n",
    "   1. [Vectorization & Embeddings](#vect)  \n",
    "   2. [Train & Evaluate Classifier](#train)  \n",
    "6. [Sentence Tagging & Extraction](#tagging)  \n",
    "   1. [Heuristic Labeling](#heuristic)  \n",
    "   2. [Build Training Examples](#examples)  \n",
    "   3. [Fine-Tune BERT](#finetune)  \n",
    "   4. [Classify Sentences](#sentclass)  \n",
    "   5. [Batch Extraction](#batchextract)  \n",
    "7. [Save Results](#save)  \n",
    "8. [Next Steps & Extensions](#next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. Environment Setup\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "# NLP & ML libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Configure logging level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Paths & Data Loading\n",
    "\n",
    "# Filepaths to labeled CSVs\n",
    "batch1_labeled_path = r\"D:\\vc-research\\vc-research\\URAP VC Research - [Readable] Batch 1 Main.csv\"\n",
    "batch1_lp_path      = r\"D:\\vc-research\\vc-research\\URAP VC Research - Batch 1 Details.csv\"\n",
    "\n",
    "# Folder containing the .txt documents\n",
    "txt_folder_path     = r\"D:\\vc-research\\vc-research\\Batch1_text_readable\"\n",
    "\n",
    "# Load the metadata and labels\n",
    "batch1_labeled = pd.read_csv(batch1_labeled_path)\n",
    "batch1_lp      = pd.read_csv(batch1_lp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DataFrame Preparation\n",
    "\n",
    "def prepare_multiindex(df, date_col=\"Date\", group_cols=[\"Company Name\"]):\n",
    "    \"\"\"\n",
    "    Convert 'Date' to datetime, set a multi-index of [Company Name, Date],\n",
    "    and sort chronologically within each company.\n",
    "    \"\"\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.set_index(group_cols + [date_col]).sort_index()\n",
    "    # Sort only by date within each company\n",
    "    df = df.groupby(level=0, sort=False).apply(lambda x: x.sort_index(level=1))\n",
    "    # Drop the outer index level (company) to leave Date as the index\n",
    "    df.index = df.index.droplevel(0)\n",
    "    return df\n",
    "\n",
    "# Apply multi-index to both DataFrames\n",
    "batch1_labeled_mi = prepare_multiindex(batch1_labeled)\n",
    "batch1_lp_mi      = prepare_multiindex(batch1_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Text Preprocessing & Label Loading\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text to lowercase, collapse newlines and extra spaces.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Load each labeled document’s text and its binary label\n",
    "text_data, labels, doc_names = [], [], []\n",
    "for _, row in batch1_labeled.iterrows():\n",
    "    fname = row['File Name']\n",
    "    label = row['Contains Liquidity Preference']\n",
    "    path  = os.path.join(txt_folder_path, fname + \".txt\")\n",
    "    if os.path.exists(path):\n",
    "        raw = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "        text_data.append(clean_text(raw))\n",
    "        labels.append(label)\n",
    "        doc_names.append(fname)\n",
    "    else:\n",
    "        logging.warning(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Vectorization & Embeddings\n",
    "\n",
    "# 1) Split into train/test preserving label proportions\n",
    "X_train, X_test, y_train, y_test, train_docs, test_docs = train_test_split(\n",
    "    text_data, labels, doc_names,\n",
    "    test_size=0.25, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# 2) TF-IDF on 1–3 grams, limit to top 2,000 features\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=2000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "# 3) Sentence-Transformer embeddings (mean pooling of sentence vectors)\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def embed_docs(docs):\n",
    "    # Tokenize into sentences, encode each, then average\n",
    "    return np.vstack([\n",
    "        bert_model.encode(sent_tokenize(d), convert_to_numpy=True).mean(axis=0)\n",
    "        for d in docs\n",
    "    ])\n",
    "\n",
    "X_train_bert = embed_docs(X_train)\n",
    "X_test_bert  = embed_docs(X_test)\n",
    "\n",
    "# 4) Combine TF-IDF and BERT embeddings horizontally\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_bert])\n",
    "X_test_combined  = np.hstack([X_test_tfidf.toarray(), X_test_bert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Confidence (Prob. of 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48_2013-12-06_Certificates of Incorporation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27_2004-08-17_Certificates of Incorporation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27_2006-08-30_Certificates of Incorporation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24_2004-12-01_Certificates of Incorporation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16_2015-04-22_Certificates of Incorporation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Document  True Label  Predicted Label  \\\n",
       "0  48_2013-12-06_Certificates of Incorporation           1                1   \n",
       "1  27_2004-08-17_Certificates of Incorporation           0                0   \n",
       "2  27_2006-08-30_Certificates of Incorporation           1                1   \n",
       "3  24_2004-12-01_Certificates of Incorporation           0                0   \n",
       "4  16_2015-04-22_Certificates of Incorporation           0                0   \n",
       "\n",
       "   Confidence (Prob. of 1)  \n",
       "0                     0.98  \n",
       "1                     0.00  \n",
       "2                     0.95  \n",
       "3                     0.08  \n",
       "4                     0.01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2 Train & Evaluate Classifier\n",
    "\n",
    "# Hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators':      [100, 200],\n",
    "    'max_depth':         [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Balanced RandomForest with 5-fold CV\n",
    "rf_cv = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight=\"balanced\"),\n",
    "    param_grid, cv=5, n_jobs=-1\n",
    ")\n",
    "rf_cv.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred      = rf_cv.best_estimator_.predict(X_test_combined)\n",
    "y_pred_prob = rf_cv.best_estimator_.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "# Build results DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Document':             test_docs,\n",
    "    'True Label':           y_test,\n",
    "    'Predicted Label':      y_pred,\n",
    "    'Confidence (Prob. of 1)': y_pred_prob\n",
    "})\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 22\n",
      "False Positive Rate: 0.0000\n",
      "False Negative Rate: 0.0667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Count correct predictions\n",
    "correct = (predictions_df['True Label'] == predictions_df['Predicted Label']).sum()\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "\n",
    "# False Positives: predicted 1 but true is 0\n",
    "false_positives = ((predictions_df['True Label'] == 0) & (predictions_df['Predicted Label'] == 1)).sum()\n",
    "# False Negatives: predicted 0 but true is 1\n",
    "false_negatives = ((predictions_df['True Label'] == 1) & (predictions_df['Predicted Label'] == 0)).sum()\n",
    "\n",
    "# Total positives and negatives for rate calculation\n",
    "total_actual_positives = (predictions_df['True Label'] == 1).sum()\n",
    "total_actual_negatives = (predictions_df['True Label'] == 0).sum()\n",
    "\n",
    "# Calculate rates\n",
    "false_positive_rate = false_positives / total_actual_negatives if total_actual_negatives > 0 else float('nan')\n",
    "false_negative_rate = false_negatives / total_actual_positives if total_actual_positives > 0 else float('nan')\n",
    "\n",
    "print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tagging & Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_2007-02-22_Certificates of Incorporation.txt</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>Company Name, Date, Document Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_2007-02-22_Certificates of Incorporation.txt</td>\n",
       "      <td>The corporation was originally incorporated un...</td>\n",
       "      <td>Company Name, Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100_2007-02-22_Certificates of Incorporation.txt</td>\n",
       "      <td>B. Pursuant to Sections 242 and 245 of the Gen...</td>\n",
       "      <td>Company Name, Document Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100_2007-02-22_Certificates of Incorporation.txt</td>\n",
       "      <td>Cc, The text of the Certificate of Incorporati...</td>\n",
       "      <td>Company Name, Document Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100_2007-02-22_Certificates of Incorporation.txt</td>\n",
       "      <td>The corporation is authorized to issue two cla...</td>\n",
       "      <td>Preferred Stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Filename  \\\n",
       "0  100_2007-02-22_Certificates of Incorporation.txt   \n",
       "1  100_2007-02-22_Certificates of Incorporation.txt   \n",
       "2  100_2007-02-22_Certificates of Incorporation.txt   \n",
       "3  100_2007-02-22_Certificates of Incorporation.txt   \n",
       "4  100_2007-02-22_Certificates of Incorporation.txt   \n",
       "\n",
       "                                            Sentence  \\\n",
       "0  State of Delaware Secretary of State Division ...   \n",
       "1  The corporation was originally incorporated un...   \n",
       "2  B. Pursuant to Sections 242 and 245 of the Gen...   \n",
       "3  Cc, The text of the Certificate of Incorporati...   \n",
       "4  The corporation is authorized to issue two cla...   \n",
       "\n",
       "                              Labels  \n",
       "0  Company Name, Date, Document Type  \n",
       "1                 Company Name, Date  \n",
       "2        Company Name, Document Type  \n",
       "3        Company Name, Document Type  \n",
       "4                   Preferred Stocks  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1 Heuristic Labeling\n",
    "\n",
    "PROPERTY_TAGS = [\n",
    "    'Company Name', 'Date', 'Document Type',\n",
    "    'Preferred Stocks', 'Priority Order', 'Liquidation Value'\n",
    "]\n",
    "\n",
    "# Keywords/regex for each tag\n",
    "KEYWORDS = {\n",
    "    'Company Name':     [\"certificate of incorporation\", \"incorporated\", r\"\\bcompany name\\b\"],\n",
    "    'Date':             [\"filed\", \"effective date\", r\"\\d{2}/\\d{2}/\\d{4}\"],\n",
    "    'Document Type':    [\"certificate of amendment\", \"articles of incorporation\", \"restated\"],\n",
    "    'Preferred Stocks': [\"preferred stock\", \"series a\", \"series b\"],\n",
    "    'Priority Order':   [\"prior and in preference\", \"ranking junior\", \"paid before\"],\n",
    "    'Liquidation Value':[\"liquidation preference\", \"liquidation value\", r\"\\$\\d+(?:,\\d{3})*(?:\\.\\d{2})?\"]\n",
    "}\n",
    "\n",
    "def label_sentences_heuristically(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenize each document into sentences, then assign all tags whose\n",
    "    keywords/regex match that sentence. Returns a DataFrame.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.endswith(\".txt\"): continue\n",
    "        text = open(os.path.join(folder_path, fname), 'r', encoding='utf-8', errors='ignore').read()\n",
    "        for sent in sent_tokenize(text.replace(\"\\n\",\" \")):\n",
    "            tags = [tag for tag, kws in KEYWORDS.items()\n",
    "                    if any(re.search(kw, sent, re.IGNORECASE) for kw in kws)]\n",
    "            if tags:\n",
    "                records.append({\n",
    "                    'Filename': fname,\n",
    "                    'Sentence': sent,\n",
    "                    'Labels':  \", \".join(tags)\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Run heuristic labeling\n",
    "labeled_sent_df = label_sentences_heuristically(txt_folder_path)\n",
    "labeled_sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Build Training Examples\n",
    "\n",
    "def build_training_examples(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Create InputExample pairs ([sentence, tag], label) for fine-tuning.\n",
    "    Label = 1.0 if sentence was tagged, else 0.0.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        sent  = row['Sentence']\n",
    "        present_tags = set(row['Labels'].split(', '))\n",
    "        for tag in PROPERTY_TAGS:\n",
    "            label = 1.0 if tag in present_tags else 0.0\n",
    "            examples.append(InputExample(texts=[sent, tag], label=label))\n",
    "    return examples\n",
    "\n",
    "training_examples = build_training_examples(labeled_sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d7c9ea5a9e4045ba8c2d6221ab4b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1034' max='1034' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1034/1034 1:32:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6.3 Fine-Tune BERT\n",
    "\n",
    "def fine_tune_bert_model(base_model, examples, epochs=1, warmup_steps=5):\n",
    "    \"\"\"\n",
    "    Fine-tune the SentenceTransformer on our tagging examples\n",
    "    using a cosine-similarity loss.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(examples, shuffle=True, batch_size=32)\n",
    "    loss_fn    = losses.CosineSimilarityLoss(model=base_model)\n",
    "    base_model.fit(\n",
    "        train_objectives=[(dataloader, loss_fn)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "fine_tuned_model = fine_tune_bert_model(bert_model, training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Classify Sentences\n",
    "\n",
    "def classify_sentences(sentences: list, model, threshold: float=0.5) -> dict:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between each sentence and each tag.\n",
    "    Return dict[tag] = list of (sentence, score) above threshold.\n",
    "    \"\"\"\n",
    "    out = defaultdict(list)\n",
    "    for sent in sentences:\n",
    "        vec_sent = model.encode(sent, convert_to_numpy=True)\n",
    "        for tag in PROPERTY_TAGS:\n",
    "            vec_tag = model.encode(tag, convert_to_numpy=True)\n",
    "            score   = util.cos_sim(vec_sent, vec_tag)[0][0].item()\n",
    "            if score >= threshold:\n",
    "                out[tag].append((sent, score))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Preferred Stocks</th>\n",
       "      <th>Priority Order</th>\n",
       "      <th>Liquidation Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_2007-02-22_Certificates of Incorporation.txt</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>The corporation was originally incorporated un...</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>The corporation is authorized to issue two cla...</td>\n",
       "      <td></td>\n",
       "      <td>The total number of shares of Common Stock whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_2008-12-03_Certificates of Incorporation.txt</td>\n",
       "      <td>State of Delaware Secre of State  Division of ...</td>\n",
       "      <td>The corporation was originally incorporated un...</td>\n",
       "      <td>State of Delaware Secre of State  Division of ...</td>\n",
       "      <td>The corporation is authorized to issue two cla...</td>\n",
       "      <td></td>\n",
       "      <td>The total number of shares of Common Stock whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16_2003-07-03_Certificates of Incorporation.txt</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>FOURTH: The total number of shares of stock wh...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16_2004-01-22_Certificates of Incorporation.txt</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>State of Delaware Secretary of State Division ...</td>\n",
       "      <td>SECOND: The corporation has not received any p...</td>\n",
       "      <td></td>\n",
       "      <td>ARTICLE IV  This corporation is authorized to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16_2004-07-14_Certificates of Incorporation.txt</td>\n",
       "      <td>RESTATED CERTIFICATE OF INCORPORATION OF 3-D M...</td>\n",
       "      <td>RESTATED CERTIFICATE OF INCORPORATION OF 3-D M...</td>\n",
       "      <td>RESTATED CERTIFICATE OF INCORPORATION OF 3-D M...</td>\n",
       "      <td>Authorization of Stock.; This corporation is a...</td>\n",
       "      <td></td>\n",
       "      <td>The total number of  GDSVF&amp;H\\567653.3  shares ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Filename  \\\n",
       "0  100_2007-02-22_Certificates of Incorporation.txt   \n",
       "1  100_2008-12-03_Certificates of Incorporation.txt   \n",
       "2   16_2003-07-03_Certificates of Incorporation.txt   \n",
       "3   16_2004-01-22_Certificates of Incorporation.txt   \n",
       "4   16_2004-07-14_Certificates of Incorporation.txt   \n",
       "\n",
       "                                        Company Name  \\\n",
       "0  State of Delaware Secretary of State Division ...   \n",
       "1  State of Delaware Secre of State  Division of ...   \n",
       "2  State of Delaware Secretary of State Division ...   \n",
       "3  State of Delaware Secretary of State Division ...   \n",
       "4  RESTATED CERTIFICATE OF INCORPORATION OF 3-D M...   \n",
       "\n",
       "                                                Date  \\\n",
       "0  The corporation was originally incorporated un...   \n",
       "1  The corporation was originally incorporated un...   \n",
       "2  State of Delaware Secretary of State Division ...   \n",
       "3  State of Delaware Secretary of State Division ...   \n",
       "4  RESTATED CERTIFICATE OF INCORPORATION OF 3-D M...   \n",
       "\n",
       "                                       Document Type  \\\n",
       "0  State of Delaware Secretary of State Division ...   \n",
       "1  State of Delaware Secre of State  Division of ...   \n",
       "2  State of Delaware Secretary of State Division ...   \n",
       "3  State of Delaware Secretary of State Division ...   \n",
       "4  RESTATED CERTIFICATE OF INCORPORATION OF 3-D M...   \n",
       "\n",
       "                                    Preferred Stocks Priority Order  \\\n",
       "0  The corporation is authorized to issue two cla...                  \n",
       "1  The corporation is authorized to issue two cla...                  \n",
       "2  FOURTH: The total number of shares of stock wh...                  \n",
       "3  SECOND: The corporation has not received any p...                  \n",
       "4  Authorization of Stock.; This corporation is a...                  \n",
       "\n",
       "                                   Liquidation Value  \n",
       "0  The total number of shares of Common Stock whi...  \n",
       "1  The total number of shares of Common Stock whi...  \n",
       "2                                                     \n",
       "3  ARTICLE IV  This corporation is authorized to ...  \n",
       "4  The total number of  GDSVF&H\\567653.3  shares ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.5 Batch Extraction of Tagged Sentences\n",
    "\n",
    "def process_directory_with_model(folder_path: str, model, threshold: float=0.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each text file in folder, extract all sentences that the model\n",
    "    tags for each property. Returns a DataFrame of one row per document.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.endswith(\".txt\"): continue\n",
    "        text      = open(os.path.join(folder_path, fname), 'r', encoding='utf-8', errors='ignore').read()\n",
    "        sentences = sent_tokenize(text.replace(\"\\n\",\" \"))\n",
    "        tagged    = classify_sentences(sentences, model, threshold)\n",
    "        row       = {'Filename': fname}\n",
    "        # Join all matching sentences per tag\n",
    "        for tag in PROPERTY_TAGS:\n",
    "            row[tag] = \"; \".join([s for s, _ in tagged.get(tag, [])])\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run the full extraction\n",
    "extracted_df = process_directory_with_model(txt_folder_path, fine_tuned_model)\n",
    "extracted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved extracted sentences to Extracted Sentences - Batch 1.csv\n"
     ]
    }
   ],
   "source": [
    "# 7. Save Results\n",
    "\n",
    "# Save the tagged sentences for manual review and downstream use\n",
    "out_csv = 'Extracted Sentences - Batch 1.csv'\n",
    "extracted_df.to_csv(out_csv, index=False)\n",
    "print(f\"✅ Saved extracted sentences to {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps & Extensions\n",
    "\n",
    "- **Error Logging:** capture parsing or model errors to a log file.  \n",
    "- **Cross-Validation:** validate sentence tagging threshold and model performance.  \n",
    "- **Parallel Processing:** speed up embedding & extraction with multiprocessing.  \n",
    "- **Edge Cases:** refine KEYWORDS and regex for better coverage of rare phrasing.  \n",
    "- **Integration:** combine document-level classification and sentence-level extraction into a unified pipeline or API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Prompt:\n",
    "\n",
    "Based on the strings in each of the cells, isolate just the desired information as describe below: \n",
    "File Name: Do not modify values in this column\n",
    "Company Name: Identify and extract the company's name as a string type (Example: \"3VR Security INC.\", \"The 41st Parameter INC.\", etc.)\n",
    "Date: Identify and extract the date when the article was filed as a datetime type (Example: \"FILED 10:43 AM 06/28/2007\", \"FILED 05:05 PM 06/10/2010\", etc.)\n",
    "Document Type: Identify and extract the type of document that was submitted as a string type (Example: \"Certificate of Incorporation\", \"Amended and Restated Certificate of Incorporation\", etc.) \n",
    "Preferred Stock: Identify and extract the unique types of preferred shares as a list of strings (Example: ['Series A', 'Series B', 'Series C', 'Series D'])\n",
    "Liquidation Value: Identify and extract the dollar liquidation amount for each preferred stock as a list of floats; the length of the list should be the same length as the list for preferred stocks; if the liquidation preference is the original issue price use that value (Example: [0.431469, 0.624136, 0.474550, 0.152430])\n",
    "\n",
    "Return the result after this extraction in the form of a dataframe and then export as a CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
