{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f0050c",
   "metadata": {},
   "source": [
    "# Certificate Field Extraction via Token Classification\n",
    "\n",
    "**Purpose:**  \n",
    "Fine-tune a SciBERT token-classification model to extract key fields from text versions of incorporation certificates:\n",
    "\n",
    "- **Company Name**  \n",
    "- **Date**  \n",
    "- **Document Type**  \n",
    "- **Preferred Stocks**  \n",
    "- **Priority Order**  \n",
    "- **Liquidation Value**\n",
    "\n",
    "**Inputs:**  \n",
    "- Metadata CSVs:  \n",
    "  - `VC Research (Batch 2) - Batch 2 Main.csv`  \n",
    "  - `VC Research (Batch 2) - Key for Data.csv`  \n",
    "- Plain-text files in `Batch2_text_readable/`\n",
    "\n",
    "**Outputs:**  \n",
    "1. A trained SciBERT token-classification model.  \n",
    "2. Quantitative evaluation metrics (precision, recall, F1).  \n",
    "3. Qualitative NER output via a Transformers pipeline.  \n",
    "4. DataFrame comparing ground-truth vs. predicted field values.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup & Imports](#setup)  \n",
    "2. [Paths & Configuration](#config)  \n",
    "3. [Data Loading & Filtering](#load)  \n",
    "4. [Span Generation for Ground Truth](#spans)  \n",
    "5. [Tokenization & Dataset Preparation](#dataset)  \n",
    "6. [Model Initialization & Training](#train)  \n",
    "7. [Evaluation & Qualitative Inference](#eval)  \n",
    "8. [Compare True vs. Predicted Fields](#compare)  \n",
    "9. [Next Steps & Extensions](#next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd482cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment Setup & Imports\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd               # for DataFrame operations\n",
    "import torch                      # for tensors and model operations\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Hugging Face Transformers for token-classification\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Configure logging (optional)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d89167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Paths & Configuration\n",
    "\n",
    "# Paths to metadata CSVs\n",
    "MAIN_CSV   = Path(r\"D:\\vc-research\\vc-research\\VC Research (Batch 2) - Batch 2 Main.csv\")\n",
    "KEY_CSV    = Path(r\"D:\\vc-research\\vc-research\\VC Research (Batch 2) - Key for Data.csv\")\n",
    "\n",
    "# Directory of readable text files from Batch 2\n",
    "TXT_DIR    = Path(r\"D:\\vc-research\\vc-research\\Batch2_text_readable\")\n",
    "\n",
    "# Pretrained model for token classification\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Loading & Filtering\n",
    "\n",
    "# Load main metadata file\n",
    "df_main = pd.read_csv(MAIN_CSV)\n",
    "df_main.columns = df_main.columns.str.strip()\n",
    "\n",
    "def find_filename_column(cols):\n",
    "    \"\"\"\n",
    "    Identify which column contains filenames; default to 'File Name'\n",
    "    or any column containing 'file' in its name.\n",
    "    \"\"\"\n",
    "    if 'File Name' in cols:\n",
    "        return 'File Name'\n",
    "    for c in cols:\n",
    "        if 'file' in c.lower():\n",
    "            return c\n",
    "    if 'Unnamed: 0' in cols:\n",
    "        return 'Unnamed: 0'\n",
    "    raise KeyError(f\"No filename column found. Columns: {cols}\")\n",
    "\n",
    "# Create a 'fname' column matching our .txt filenames\n",
    "file_col = find_filename_column(df_main.columns)\n",
    "df_main['fname'] = (\n",
    "    df_main[file_col]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .apply(lambda x: Path(x).stem + '.txt')\n",
    ")\n",
    "\n",
    "# Read in all text files into a dict: { filename : full_text }\n",
    "txt_files = list(TXT_DIR.glob(\"*.txt\"))\n",
    "texts = {p.name: p.read_text(encoding='utf-8', errors='ignore') for p in txt_files}\n",
    "\n",
    "# Filter metadata to only those with an existing text file\n",
    "df = df_main[df_main['fname'].isin(texts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac93b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Span Generation for Ground Truth\n",
    "\n",
    "def find_span(text: str, value: str):\n",
    "    \"\"\"\n",
    "    Return (start, end) indices of the first occurrence of `value` in `text`,\n",
    "    or None if not found.\n",
    "    \"\"\"\n",
    "    idx = text.find(value)\n",
    "    return (idx, idx + len(value)) if idx >= 0 else None\n",
    "\n",
    "# Prepare training examples: each with text + lists of character spans + labels\n",
    "examples = []\n",
    "FIELDS = [\n",
    "    'Company Name', 'Date', 'Document Type',\n",
    "    'Preferred Stocks', 'Priority Order', 'Liquidation Value'\n",
    "]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    doc_text = texts[row['fname']]\n",
    "    span_starts, span_ends, span_labels = [], [], []\n",
    "\n",
    "    for field in FIELDS:\n",
    "        value = row.get(field)\n",
    "        if pd.isna(value):\n",
    "            continue\n",
    "        # Handle comma-separated multiple values\n",
    "        tokens = str(value).split(',') if ',' in str(value) else [str(value)]\n",
    "        for tok in tokens:\n",
    "            tok = tok.strip()\n",
    "            span = find_span(doc_text, tok)\n",
    "            if span:\n",
    "                s, e = span\n",
    "                span_starts.append(s)\n",
    "                span_ends.append(e)\n",
    "                # Replace spaces in label with underscore for BIO tagging\n",
    "                span_labels.append(field.replace(' ', '_'))\n",
    "\n",
    "    # Only include docs where at least one span was found\n",
    "    if span_starts:\n",
    "        examples.append({\n",
    "            'text': doc_text,\n",
    "            'span_starts': span_starts,\n",
    "            'span_ends': span_ends,\n",
    "            'span_labels': span_labels\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aab0b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458209c963ee4fc787bc7c3ba7e4b0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Owner\\.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f65aa38963d453ca7aa2e163baace5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Tokenization & Dataset Preparation\n",
    "\n",
    "# Load model config to get max sequence length\n",
    "config    = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "max_len   = config.max_position_embeddings or 512\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Build label vocabulary in BIO format\n",
    "unique_fields = sorted({lbl for ex in examples for lbl in ex['span_labels']})\n",
    "bio_labels    = ['O'] + [f\"{p}-{fld}\" for fld in unique_fields for p in ('B','I')]\n",
    "label2id      = {lab: i for i, lab in enumerate(bio_labels)}\n",
    "id2label      = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "# Tokenize and align labels to token offsets\n",
    "encodings = []\n",
    "for ex in examples:\n",
    "    enc = tokenizer(\n",
    "        ex['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    offsets = enc.pop('offset_mapping')\n",
    "    labels = [label2id['O']] * max_len\n",
    "\n",
    "    # Assign B- and I-labels to tokens overlapping each ground-truth span\n",
    "    for start, end, fld in zip(ex['span_starts'], ex['span_ends'], ex['span_labels']):\n",
    "        for i, (off_s, off_e) in enumerate(offsets):\n",
    "            if off_e <= start:\n",
    "                continue\n",
    "            if off_s >= end:\n",
    "                break\n",
    "            tag = 'B' if off_s == start else 'I'\n",
    "            labels[i] = label2id[f\"{tag}-{fld}\"]\n",
    "\n",
    "    enc['labels'] = labels\n",
    "    encodings.append(enc)\n",
    "\n",
    "# Custom Dataset wrapping our encodings\n",
    "class NERDataset(TorchDataset):\n",
    "    def __init__(self, encs): \n",
    "        self.encs = encs\n",
    "    def __len__(self): \n",
    "        return len(self.encs)\n",
    "    def __getitem__(self, idx): \n",
    "        return {k: torch.tensor(v) for k, v in self.encs[idx].items()}\n",
    "\n",
    "# Split into train/eval\n",
    "train_encs, eval_encs = train_test_split(encodings, test_size=0.1, random_state=42)\n",
    "train_dataset = NERDataset(train_encs)\n",
    "eval_dataset  = NERDataset(eval_encs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc47d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64946865533f4183b99d4095692b7c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_2756\\4248643483.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a0365a5dc84d07835bcb60e655d521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 08:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84, training_loss=0.07871079870632716, metrics={'train_runtime': 532.743, 'train_samples_per_second': 0.625, 'train_steps_per_second': 0.158, 'total_flos': 87014179998720.0, 'train_loss': 0.07871079870632716, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Model Initialization & Training\n",
    "\n",
    "# Load pre-trained SciBERT for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(bio_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Collator that pads inputs & labels\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='out_ner',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# Trainer API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab2e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trainer.evaluate() metrics ===\n",
      "{'eval_loss': 0.026672448962926865, 'eval_runtime': 3.3936, 'eval_samples_per_second': 3.831, 'eval_steps_per_second': 1.179, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seqeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_metrics)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Compute span-level precision/recall/F1 with seqeval\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score, classification_report\n\u001b[0;32m     11\u001b[0m pred_logits, true_label_ids, _ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(eval_dataset)\n\u001b[0;32m     12\u001b[0m pred_label_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39mtensor(pred_logits), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seqeval'"
     ]
    }
   ],
   "source": [
    "# 7. Evaluation & Qualitative Inference\n",
    "\n",
    "# 7.1 Quantitative Evaluation\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"=== Trainer.evaluate() metrics ===\")\n",
    "print(eval_metrics)\n",
    "\n",
    "# Compute span-level precision/recall/F1 with seqeval\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "pred_logits, true_label_ids, _ = trainer.predict(eval_dataset)\n",
    "pred_label_ids = torch.argmax(torch.tensor(pred_logits), dim=-1).tolist()\n",
    "\n",
    "true_tags = [[id2label[i] for i in seq] for seq in true_label_ids]\n",
    "pred_tags = [[id2label[i] for i in seq] for seq in pred_label_ids]\n",
    "\n",
    "print(\"\\n=== Span-level Metrics ===\")\n",
    "print(\"Precision:\", precision_score(true_tags, pred_tags))\n",
    "print(\"Recall:   \", recall_score(true_tags, pred_tags))\n",
    "print(\"F1:       \", f1_score(true_tags, pred_tags))\n",
    "print(\"\\n\", classification_report(true_tags, pred_tags))\n",
    "\n",
    "# 7.2 Qualitative Inference via NER Pipeline\n",
    "tokenizer.model_max_length = max_len\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def extract_fields(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Run the NER pipeline on raw text and aggregate tokens by field.\n",
    "    \"\"\"\n",
    "    entities = ner_pipe(text)\n",
    "    fields = defaultdict(list)\n",
    "    for ent in entities:\n",
    "        grp = ent.get(\"entity_group\", ent.get(\"entity\"))\n",
    "        if '-' in grp:\n",
    "            tag, fld = grp.split('-', 1)\n",
    "        else:\n",
    "            fld = grp\n",
    "        fields[fld].append(ent[\"word\"])\n",
    "    return {fld: \" \".join(tokens) for fld, tokens in fields.items()}\n",
    "\n",
    "print(\"\\n=== Sample Inference Results ===\")\n",
    "for fname, raw in texts.items():\n",
    "    print(f\"--- {fname} ---\")\n",
    "    out = extract_fields(raw)\n",
    "    if out:\n",
    "        for fld, txt in out.items():\n",
    "            print(f\" {fld}: {txt}\")\n",
    "    else:\n",
    "        print(\" (no entities found)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5db46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>Company_Name_true</th>\n",
       "      <th>Company_Name_pred</th>\n",
       "      <th>Date_true</th>\n",
       "      <th>Date_pred</th>\n",
       "      <th>Document_Type_true</th>\n",
       "      <th>Document_Type_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223_2007-08-03_Certificates of Incorporation.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advion biosciences inc. .</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192_2005-09-27_Certificates of Incorporation.txt</td>\n",
       "      <td>Advanced BioHealing, Inc.</td>\n",
       "      <td>advanced biohealing inc. .</td>\n",
       "      <td>2005-09-27</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189_2005-12-20_Certificates of Incorporation.txt</td>\n",
       "      <td>Adspace Networks, Inc.</td>\n",
       "      <td>. .</td>\n",
       "      <td>2005-12-20</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200_2008-08-22_Certificates of Incorporation.txt</td>\n",
       "      <td>Advanced Electron Beams, Inc.</td>\n",
       "      <td>advanced electron beams inc.</td>\n",
       "      <td>2008-08-22</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporation</td>\n",
       "      <td>amended and restated certificate of incorporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188_2010-11-08_Certificates of Incorporation.txt</td>\n",
       "      <td>Semantic Sugar, Inc.</td>\n",
       "      <td>semantic sugar inc.</td>\n",
       "      <td>2010-11-08</td>\n",
       "      <td></td>\n",
       "      <td>Certificate of Amendment to the Restated Certi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>181_2007-10-29_Certificates of Incorporation.txt</td>\n",
       "      <td>Adknowledge, Inc.</td>\n",
       "      <td>adknowledge inc.</td>\n",
       "      <td>2007-10-29</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporat...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200_2013-01-30_Certificates of Incorporation.txt</td>\n",
       "      <td>Advanced Electron Beams, Inc.</td>\n",
       "      <td>advanced electron beams inc.</td>\n",
       "      <td>2013-01-30</td>\n",
       "      <td></td>\n",
       "      <td>Certificate of Dissolution</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>234_2012-02-14_Certificates of Incorporation.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aerohive networks inc. ##oh networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>136_2007-02-14_Certificates of Incorporation.txt</td>\n",
       "      <td>Actmis Pharamaceuticals, Inc.</td>\n",
       "      <td>. inc.</td>\n",
       "      <td>2007-02-14</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>169_2011-08-16_Certificates of Incorporation.txt</td>\n",
       "      <td>Adchemy, Inc.</td>\n",
       "      <td>adchemy inc.</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>193_2011-05-09_Certificates of Incorporation.txt</td>\n",
       "      <td>Advanced BioNutrition Corp.</td>\n",
       "      <td>advanced bionutrition corp</td>\n",
       "      <td>2011-05-09</td>\n",
       "      <td></td>\n",
       "      <td>Articles of Amendment and Restatement</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>132_2012-04-18_Certificates of Incorporation.txt</td>\n",
       "      <td>Acronis, INC</td>\n",
       "      <td>acronis inc.</td>\n",
       "      <td>2012-04-18</td>\n",
       "      <td></td>\n",
       "      <td>Certificate of Amendment</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>188_2014-04-14_Certificates of Incorporation.txt</td>\n",
       "      <td>Semantic Sugar, Inc.</td>\n",
       "      <td>semantic sugar inc.</td>\n",
       "      <td>2014-04-14</td>\n",
       "      <td></td>\n",
       "      <td>Amended and Restated Certificate of Incorporation</td>\n",
       "      <td>amended and restated certificate of incorporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>189_2010-03-01_Certificates of Incorporation.txt</td>\n",
       "      <td>Adspace Networks, Inc.</td>\n",
       "      <td>adspace networks inc.</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td></td>\n",
       "      <td>Certificate of Change of Registered Agent and/...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname  \\\n",
       "0   223_2007-08-03_Certificates of Incorporation.txt   \n",
       "1   192_2005-09-27_Certificates of Incorporation.txt   \n",
       "2   189_2005-12-20_Certificates of Incorporation.txt   \n",
       "3   200_2008-08-22_Certificates of Incorporation.txt   \n",
       "4   188_2010-11-08_Certificates of Incorporation.txt   \n",
       "5   181_2007-10-29_Certificates of Incorporation.txt   \n",
       "6   200_2013-01-30_Certificates of Incorporation.txt   \n",
       "7   234_2012-02-14_Certificates of Incorporation.txt   \n",
       "8   136_2007-02-14_Certificates of Incorporation.txt   \n",
       "9   169_2011-08-16_Certificates of Incorporation.txt   \n",
       "10  193_2011-05-09_Certificates of Incorporation.txt   \n",
       "11  132_2012-04-18_Certificates of Incorporation.txt   \n",
       "12  188_2014-04-14_Certificates of Incorporation.txt   \n",
       "13  189_2010-03-01_Certificates of Incorporation.txt   \n",
       "\n",
       "                Company_Name_true                     Company_Name_pred  \\\n",
       "0                             NaN             advion biosciences inc. .   \n",
       "1       Advanced BioHealing, Inc.            advanced biohealing inc. .   \n",
       "2          Adspace Networks, Inc.                                   . .   \n",
       "3   Advanced Electron Beams, Inc.          advanced electron beams inc.   \n",
       "4            Semantic Sugar, Inc.                   semantic sugar inc.   \n",
       "5               Adknowledge, Inc.                      adknowledge inc.   \n",
       "6   Advanced Electron Beams, Inc.          advanced electron beams inc.   \n",
       "7                             NaN  aerohive networks inc. ##oh networks   \n",
       "8   Actmis Pharamaceuticals, Inc.                                . inc.   \n",
       "9                  Adchemy, Inc.                           adchemy inc.   \n",
       "10    Advanced BioNutrition Corp.            advanced bionutrition corp   \n",
       "11                   Acronis, INC                          acronis inc.   \n",
       "12           Semantic Sugar, Inc.                   semantic sugar inc.   \n",
       "13         Adspace Networks, Inc.                 adspace networks inc.   \n",
       "\n",
       "     Date_true Date_pred                                 Document_Type_true  \\\n",
       "0          NaN                                                          NaN   \n",
       "1   2005-09-27            Amended and Restated Certificate of Incorporation   \n",
       "2   2005-12-20            Amended and Restated Certificate of Incorporation   \n",
       "3   2008-08-22            Amended and Restated Certificate of Incorporation   \n",
       "4   2010-11-08            Certificate of Amendment to the Restated Certi...   \n",
       "5   2007-10-29            Amended and Restated Certificate of Incorporat...   \n",
       "6   2013-01-30                                   Certificate of Dissolution   \n",
       "7          NaN                                                          NaN   \n",
       "8   2007-02-14            Amended and Restated Certificate of Incorporation   \n",
       "9   2011-08-16            Amended and Restated Certificate of Incorporation   \n",
       "10  2011-05-09                        Articles of Amendment and Restatement   \n",
       "11  2012-04-18                                     Certificate of Amendment   \n",
       "12  2014-04-14            Amended and Restated Certificate of Incorporation   \n",
       "13  2010-03-01            Certificate of Change of Registered Agent and/...   \n",
       "\n",
       "                                   Document_Type_pred  \n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3   amended and restated certificate of incorporation  \n",
       "4                                                      \n",
       "5                                                      \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                                      \n",
       "9                                                      \n",
       "10                                                     \n",
       "11                                                     \n",
       "12  amended and restated certificate of incorporation  \n",
       "13                                                     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Compare True vs. Predicted Fields\n",
    "\n",
    "# Identify which fields we have ground-truth columns for\n",
    "candidate = [\n",
    "    'Company Name','Date','Document Type',\n",
    "    'Preferred Stocks','Priority Order','Liquidation Value'\n",
    "]\n",
    "orig_fields = [f for f in candidate if f in df.columns]\n",
    "pred_fields = [f.replace(' ', '_') for f in orig_fields]\n",
    "\n",
    "# Select a subset of files for evaluation\n",
    "eval_files = train_test_split(df['fname'], test_size=0.9, random_state=42)[0]\n",
    "\n",
    "# Build comparison rows\n",
    "rows = []\n",
    "for fname in eval_files:\n",
    "    true_row = df[df['fname']==fname].iloc[0]\n",
    "    pred_row = extract_fields(texts[fname])\n",
    "    row = {'fname': fname}\n",
    "    for orig, lab in zip(orig_fields, pred_fields):\n",
    "        row[f\"{lab}_true\"] = true_row.get(orig, \"\")\n",
    "        row[f\"{lab}_pred\"] = pred_row.get(lab, \"\")\n",
    "    rows.append(row)\n",
    "\n",
    "df_eval = pd.DataFrame(rows)\n",
    "display(df_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7acf59",
   "metadata": {},
   "source": [
    "## Next Steps & Extensions\n",
    "\n",
    "- **Logging & Error Handling:** record missing spans or tokenization errors.  \n",
    "- **Hyperparameter Tuning:** experiment with learning rates, batch sizes, epochs.  \n",
    "- **Thresholding:** refine NER pipeline aggregation & threshold settings.  \n",
    "- **Integration:** wrap into an API for automated certificate processing.  \n",
    "- **Unit Tests:** validate span alignment and label correctness with pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a9b2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
